{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "037316f6-d705-4303-8966-e2a45ee3324b",
   "metadata": {},
   "source": [
    "#第一章 1.3 Jieba分词"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22347793-6ef0-4821-8b2c-bb12ac9fe7c6",
   "metadata": {},
   "source": [
    "##1.3.1 基于N-gram模型的中文分词"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5cdc19-8d79-4add-84d2-c9cc2cbe7947",
   "metadata": {},
   "source": [
    "###N-gram模型原理及其在分词中的应用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0faed4fb-795b-47e7-9438-8f3065b50d1b",
   "metadata": {},
   "source": [
    "N-gram是一种基于统计的语言模型，假设当前词的出现概率仅依赖于前N-1个词。\n",
    "在分词中的应用\n",
    "**二元模型（Bigram）**最常用，计算相邻词的共现概率：\n",
    "通过统计语料库中词序列频率，选择概率最大的分词组合。例如：\n",
    "\"研究生命\"可切分为\"研究/生命\"（P=0.8）或\"研究生/命\"（P=0.2），模型会选择前者。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b756703a-7522-46dd-964d-3dea6519bae6",
   "metadata": {},
   "source": [
    "##1.3.2 基于隐马尔可夫模型（HMM）的中文分词"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8c1970-a995-44b0-a756-dcf72f42f0b3",
   "metadata": {},
   "source": [
    "HMM建模过程\n",
    "定义状态：在分词任务中，状态通常表示词的边界，例如词的开始（B），词的结束（E），词的中间（M）和独立成词的字符（S）。\n",
    "观测序列：观测序列即为输入的中文句子。\n",
    "状态转移概率：定义状态之间转移的概率，例如从状态B转移到状态E的概率。\n",
    "发射概率：定义在特定状态下生成观测（即词或字符）的概率。\n",
    "初始状态分布：定义序列开始时各个状态的概率。\n",
    "训练模型：使用带标签的数据集训练模型，估计状态转移概率、发射概率和初始状态分布。\n",
    "解码：使用维特比算法（Viterbi Algorithm）或其他解码算法找到最有可能的状态序列，从而实现分词。\n",
    "分词实现方法\n",
    "在分词实现中，HMM可以按照以下步骤进行：\n",
    "提取特征：对输入的中文句子进行预处理，提取特征。\n",
    "建立模型：根据提取的特征建立HMM模型，定义状态、观测、状态转移概率、发射概率和初始状态分布。\n",
    "训练模型：使用带标签的数据集对模型进行训练，调整模型参数以最大化观测序列的概率。\n",
    "应用模型：将训练好的模型应用于新的观测序列，使用解码算法找到最有可能的状态序列，从而实现分词。\n",
    "输出结果：根据解码得到的状态序列，将输入的中文句子进行分词。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4432f738-a05b-4144-a9d4-de16e8a86e63",
   "metadata": {},
   "source": [
    "##1.3.3 Jieba分词原理与流程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e24687-84ba-4ad4-9bbd-2a74dcf6d581",
   "metadata": {},
   "source": [
    "原理\n",
    "Jieba分词的核心原理是利用统计语言模型来识别文本中的词语。它首先使用一个词典（通常是基于大量文本数据构建的），然后通过统计方法来确定哪些字符序列更有可能是词语。Jieba分词主要基于以下几个步骤：\n",
    "词典构建：构建一个词典，其中包含了大量的词语及其对应的频率和词性标注。\n",
    "句子扫描：对输入的句子进行扫描，生成所有可能的词语组合。\n",
    "动态规划：使用动态规划算法（如维特比算法）来寻找最优的词语切分路径，即最大概率的词语序列。\n",
    "HMM模型：对于未登录词（不在词典中的词），使用隐马尔可夫模型（HMM）来进行识别。\n",
    "流程\n",
    "加载词典：Jieba分词库自带一个词典，但也允许用户加载自定义词典来提高分词的准确性。\n",
    "获取关键词：利用Jieba的cut()和lcut()函数来获取分词结果。cut()函数生成一个生成器，而lcut()函数生成一个列表。\n",
    "去除停用词：Jieba允许用户指定停用词，这些词在分词结果中将被忽略。\n",
    "数据处理：对分词结果进行后处理，如去除停用词、进行词性标注等。\n",
    "分词模式：Jieba支持三种分词模式——精确模式、全模式和搜索引擎模式。精确模式试图将句子最精确地切开，全模式把句子中所有可能的词语都扫描出来，搜索引擎模式在精确模式的基础上，对长词再次切分，提高召回率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22cff50c-a1ed-4ea2-b620-2ea427d80286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_max_matching(vocab, sentence):\n",
    "    \"\"\"\n",
    "    基于正向最大匹配算法 (FMM) 的中文分词函数\n",
    "\n",
    "    参数：\n",
    "    vocab (list): 词典，包含了所有可能的词汇\n",
    "    sentence (str): 需要分词的句子\n",
    "\n",
    "    返回：\n",
    "    list: 分词结果的列表\n",
    "    \"\"\"\n",
    "    fmmresult = []  # 存储分词结果的列表\n",
    "    max_len = max([len(item) for item in vocab])  # 获取词典中最长词的长度\n",
    "    start = 0  # 分词的起始位置\n",
    "\n",
    "    # 开始遍历句子，直到处理完整个句子\n",
    "    while start != len(sentence):\n",
    "        index = start + max_len  # 尝试匹配最大长度的词\n",
    "        if index > len(sentence):  # 如果索引超出句子长度，修正为句子末尾\n",
    "            index = len(sentence)\n",
    "\n",
    "        # 从当前起始位置尝试从最大长度开始逐步缩小词的长度进行匹配\n",
    "        while index > start:\n",
    "            current_substr = sentence[start:index]  # 截取当前子串\n",
    "            # 如果子串在词典中，或者子串长度为1，则认为是一个有效词\n",
    "            if current_substr in vocab or len(current_substr) == 1:\n",
    "                fmmresult.append(current_substr)  # 将有效词加入结果列表\n",
    "                start = index  # 更新起始位置，跳过已处理的部分\n",
    "                break  # 找到一个有效词后跳出内层循环继续处理下一个子串\n",
    "            index -= 1  # 如果没有匹配到有效词，缩短子串长度再试\n",
    "\n",
    "    return fmmresult  # 返回最终的分词结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae8719ad-9cba-4de5-9ed3-187617afdb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_directional_max_matching(vocab, sentence):\n",
    "    \"\"\"\n",
    "    Reverse Maximum Matching (RMM) 分词算法。\n",
    "    从句子的末尾开始，尝试从词典中匹配最长的词直到句子被分割。\n",
    "\n",
    "    Args:\n",
    "    vocab (list): 词典，包含了所有可能的词汇\n",
    "    sentence (str): 需要分词的句子\n",
    "\n",
    "    返回：\n",
    "    list: 分词后的结果，按顺序返回分词列表\n",
    "    \"\"\"\n",
    "    rmmresult = []  # 存储分词结果\n",
    "    max_len = max([len(item) for item in vocab])  # 获取词典中最大词的长度\n",
    "    start = len(sentence)  # 从句子的末尾开始\n",
    "\n",
    "    while start != 0:  # 直到处理完整个句子\n",
    "        index = start - max_len  # 尝试从当前位置往前推最大长度的子串\n",
    "        if index < 0:\n",
    "            index = 0  # 防止下标越界，调整为从0开始\n",
    "\n",
    "        while index < start:  # 向前查找直到找到匹配的词\n",
    "            current_substr = sentence[index:start]  # 截取当前子串\n",
    "\n",
    "            # 如果当前子串在词典中，或当前子串长度为1（即单个字符）\n",
    "            if current_substr in vocab or len(current_substr) == 1:\n",
    "                rmmresult.insert(0, current_substr)  # 匹配成功，插入到结果列表的开头\n",
    "                start = index  # 更新起始位置，继续向前匹配\n",
    "                break  # 找到一个词后跳出内层循环\n",
    "            index += 1  # 如果当前子串没有匹配，向前移动一个字符继续尝试\n",
    "\n",
    "    return rmmresult  # 返回最终的分词结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02af4610-27fc-4f8f-a533-af22ca24ccbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bi_directional_matching(vocab, sentence):\n",
    "    # 获取正向和反向最大匹配的分词结果\n",
    "    res1 = forward_max_matching(vocab, sentence)\n",
    "    res2 = reverse_directional_max_matching(vocab, sentence)\n",
    "\n",
    "    len_res1, len_res2 = len(res1), len(res2)  # 保存长度\n",
    "\n",
    "    # 如果两个结果的长度相同\n",
    "    if len_res1 == len_res2:\n",
    "        # 如果两个结果相同，直接返回\n",
    "        if res1 == res2:\n",
    "            return res1\n",
    "        else:\n",
    "            # 统计每个结果中长度为1的词的数量\n",
    "            res1_sn = sum(1 for i in res1 if len(i) == 1)\n",
    "            res2_sn = sum(1 for i in res2 if len(i) == 1)\n",
    "            # 返回包含较少单字符词的结果\n",
    "            return res1 if res1_sn < res2_sn else res2\n",
    "    else:\n",
    "        # 返回词数较少的结果\n",
    "        return res1 if len_res1 < len_res2 else res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4211d22-2aa0-49d2-a5c5-fba21c72cee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = ['我们', '今天', '在', '野生动物园', '玩']\n",
    "sentence = '我们是今天在野生动物园玩了'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bbf81223-6e47-43bf-8345-01c7261a6a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['我们', '是', '今天', '在', '野生动物园', '玩', '了']\n",
      "['我们', '是', '今天', '在', '野生动物园', '玩', '了']\n"
     ]
    }
   ],
   "source": [
    "fmm_result = forward_max_matching(vocab, sentence)\n",
    "rmm_result = reverse_directional_max_matching(vocab, sentence)\n",
    "print(fmm_result)\n",
    "print(rmm_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8c83988-f7b9-41f8-a2d7-ae8ba950786b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['我们', '是', '今天', '在', '野生动物园', '玩', '了']\n"
     ]
    }
   ],
   "source": [
    "bm_result = bi_directional_matching(vocab, sentence)\n",
    "print(bm_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
